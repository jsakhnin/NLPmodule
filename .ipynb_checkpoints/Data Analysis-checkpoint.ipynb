{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training data might take some time. (We can load a select number of rows for quick analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('https://media.githubusercontent.com/media/jsakhnin/JigsawNLP_data/master/train.csv' )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really like the way this notebook does the data analysis and data processing\n",
    "https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw.\n",
    "\n",
    "Importing libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add new features to the dataset based on existing features (e.g. length of comment, number of capital letters, number of exclamation marks, etc). Some of this data could be correlated with toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding length of comment as a column\n",
    "train['total_length'] = train['comment_text'].apply(len)\n",
    "#Adding number of capital letters as a column\n",
    "train['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "#Adding ratio of capital letters vs length of comment\n",
    "train['caps_vs_length'] = train.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\n",
    "\n",
    "#Adding number of exclamation marks\n",
    "train['num_exclamation_marks'] = train['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "#Adding number of question marks\n",
    "train['num_question_marks'] = train['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "#Number of punctuations\n",
    "train['num_punctuation'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "#Number of symbols (*$%&)\n",
    "train['num_symbols'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "\n",
    "#Number of works in each comment\n",
    "train['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "#Number of unique words\n",
    "train['num_unique_words'] = train['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "#Percentage of unique words in a comment\n",
    "train['words_vs_unique'] = train['num_unique_words'] / train['num_words']\n",
    "\n",
    "#Number of smilie faces in the comment\n",
    "train['num_smilies'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the correlation of our newly created features with some of the original columns of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ('total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks','num_question_marks',\n",
    "            'num_punctuation', 'num_words', 'num_unique_words','words_vs_unique', 'num_smilies', 'num_symbols')\n",
    "\n",
    "columns = ('target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'funny', 'wow', 'sad',\n",
    "           'likes', 'disagree', 'sexual_explicit','identity_annotator_count', 'toxicity_annotator_count')\n",
    "\n",
    "rows = [{c:train[f].corr(train[c]) for c in columns} for f in features]\n",
    "train_correlations = pd.DataFrame(rows, index=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our corrleation table looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with new Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these correlations in a heatmap: (If we're not using the entire data, the heatmap may look incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(font_scale=1)\n",
    "ax = sns.heatmap(train_correlations, vmin=-0.1, vmax=0.1, center=0.0, cmap=\"YlGnBu\")\n",
    "figure = svm.get_figure()    \n",
    "figure.savefig('HMAP_NewFeatures_Correlation.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Identities and Comment Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities = tuple(train.iloc[:, 8:32])\n",
    "rows = [{c:train[f].corr(train[c]) for c in columns} for f in identities]\n",
    "poptoxicity_correlations = pd.DataFrame(rows, index=identities)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(font_scale=1)\n",
    "ax = sns.heatmap(poptoxicity_correlations, vmin=-0.1, vmax=0.1, center=0.0, cmap=\"YlGnBu\")\n",
    "figure = svm.get_figure()    \n",
    "figure.savefig('HMAP_Identities_Correlation.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find a rleationship between average toxicity of comments with demographic groups: (Not complete/accurate with low sample of data, we need to run this on the whole data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = train.loc[:, ['target']+list(train)[slice(8,32)]].dropna()\n",
    "weighted_toxic = demographics.iloc[:, 1:].multiply(demographics.iloc[:, 0], axis=\"index\").sum()/demographics.iloc[:, 1:][demographics.iloc[:, 1:]>0].count()\n",
    "weighted_toxic = weighted_toxic.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "sns.set(font_scale=3)\n",
    "ax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, alpha=0.8)\n",
    "plt.ylabel('Demographics')\n",
    "plt.xlabel('Weighted Toxic')\n",
    "plt.show()\n",
    "plt.savefig('Demographic_WeightedToxicity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at samples per category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.DataFrame()\n",
    "toxicCat = []\n",
    "normalCat = []\n",
    "for i in range(weighted_toxic.index.size):\n",
    "#     cat_df = train.apply(lambda x: True if ( (train['target']>0.5)  & (train[weighted_toxic.index[i]] > 0)) else False, axis=1)\n",
    "    cat_df_bad = train[ (train['target']>0.5) & (train[weighted_toxic.index[i]] > 0.0) ]\n",
    "    cat_df_good = train[ (train['target']<0.5) & (train[weighted_toxic.index[i]] > 0.0) ]\n",
    "    toxicCat.append(len(cat_df_bad))\n",
    "    normalCat.append(len(cat_df_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now \"normalCat\" and \"toxicCat\" contains the number of normal and toxic samples per category respectively. They are ordered the same as \"weighted_toxic.index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalCat)\n",
    "print(toxicCat)\n",
    "\n",
    "normalCat = np.asarray(normalCat)\n",
    "toxicCat = np.asarray(toxicCat)\n",
    "samplesPerCategory = np.vstack((normalCat, toxicCat))\n",
    "\n",
    "print(normalCat.shape)\n",
    "print(toxicCat.shape)\n",
    "print(samplesPerCategory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "sns.set(font_scale=3)\n",
    "ax = sns.barplot(x = normalCat, y = weighted_toxic.index, alpha=0.8, color=\"blue\")\n",
    "ax = sns.barplot(x = toxicCat, y = weighted_toxic.index, alpha=0.8, color=\"red\")\n",
    "plt.ylabel('Demographics')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.savefig('Demographic_Samples.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we weight the data based on dates and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withdate = train.loc[:, ['created_date', 'target']+list(train)[slice(8,32)]].dropna()\n",
    "raceweighted = withdate.iloc[:, 2:]/withdate.iloc[:, 2:].sum()\n",
    "race_target_weighted = raceweighted.multiply(withdate.iloc[:, 1], axis=\"index\")\n",
    "race_target_weighted['created_date'] = pd.to_datetime(withdate['created_date']).values.astype('datetime64[M]')\n",
    "weighted_demo = race_target_weighted.groupby(['created_date']).sum().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing Sizes of Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n",
    "              'verticalalignment':'bottom'} # Bottom vertical alignment for more space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_demo[['white', 'asian', 'black', 'jewish', 'latino', 'other_race_or_ethnicity']].plot(\n",
    "    title = 'Time Series Toxicity & Race' )\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weighted Toxicity')\n",
    "plt.legend()\n",
    "plt.savefig('Toxicity_vs_Race.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_demo[['atheist', 'buddhist', 'christian', 'hindu', 'muslim', 'other_religion']].plot(\n",
    "    title = 'Time Series Toxicity & Race' )\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weighted Toxicity')\n",
    "plt.legend()\n",
    "plt.savefig('Toxicity_vs_Religion.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_demo[['heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']].plot(\n",
    "    title = 'Time Series Toxicity & Race' )\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weighted Toxicity')\n",
    "plt.legend()\n",
    "plt.savefig('Toxicity_vs_SexualOrientation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_demo[['male', 'female', 'transgender', 'other_gender']].plot(\n",
    "    title = 'Time Series Toxicity & Race' )\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weighted Toxicity')\n",
    "plt.legend()\n",
    "plt.savefig('Toxicity_vs_Gender.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_demo[['physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']].plot(\n",
    "    title = 'Time Series Toxicity & Race' )\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weighted Toxicity')\n",
    "plt.legend()\n",
    "plt.savefig('Toxicity_vs_Disability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
